kind: ClusterServingRuntime
apiVersion: serving.kserve.io/v1alpha1
metadata:
  name: nvidia-nim-llama3-8b-instruct-1.0.3
spec:
  annotations:
    prometheus.kserve.io/path: /metrics
    prometheus.kserve.io/port: "8002"
    serving.kserve.io/enable-metric-aggregation: "true"
    serving.kserve.io/enable-prometheus-scraping: "true"
  containers:
    - image: nvcr.io/nim/meta/llama3-8b-instruct:1.0.3
      name: kserve-container
      env:
      - name: NIM_CACHE_PATH
        value: /mnt/models/cache
      - name: HF_TOKEN
        valueFrom:
          secretKeyRef:
            name: nvidia-nim-secrets
            key: HF_TOKEN
      - name: NGC_API_KEY
        valueFrom:
          secretKeyRef:
            name: nvidia-nim-secrets
            key: NGC_API_KEY
      ports:
      - containerPort: 8000
        protocol: TCP
      resources:
        limits:
          cpu: "12"
          memory: 32Gi
          nvidia.com/gpu: 1
        requests:
          cpu: "12"
          memory: 32Gi
          nvidia.com/gpu: 1
      volumeMounts:
      - mountPath: /dev/shm
        name: dshm
  {{- with .Values.imagePullSecrets }}
  imagePullSecrets:
    {{- toYaml . | nindent 4 }}
  {{- end }}
  protocolVersions:
  - v2
  - grpc-v2
  supportedModelFormats:
  - autoSelect: true
    name: nvidia-nim-llama3-8b-instruct
    priority: 1
    version: 1.0.3
  volumes:
  - emptyDir:
      medium: Memory
      sizeLimit: 16Gi    
    name: dshm
