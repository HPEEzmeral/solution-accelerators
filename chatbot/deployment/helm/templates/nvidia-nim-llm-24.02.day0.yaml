kind: ClusterServingRuntime
apiVersion: serving.kserve.io/v1alpha1
metadata:
  name: nvidia-nim-llm-24.02.day0
spec:
  annotations:
    prometheus.kserve.io/path: /metrics
    prometheus.kserve.io/port: "8002"
  containers:
  - args:
    - |
      ln -s /mnt/models/* /model-store/; \
      echo 'engine:
        model: /model-store
        tensor_parallel_size: {{`{{ .Annotations.num_gpus }}`}}
        dtype: float16' > /tmp/model_config.yaml; \
      nim_vllm --model_name={{`{{ .Annotations.nim_model_name }}`}} \
        --openai_port=8000 \
        --model_config /tmp/model_config.yaml
    command:
    - /bin/sh
    - -c
    image: nvcr.io/ohlfw0olaadg/ea-participants/nim_llm:24.02.1-day0
    name: kserve-container
    ports:
    - containerPort: 8000
      protocol: TCP
    resources:
      limits:
        cpu: "8"
        memory: 128Gi
        nvidia.com/gpu: 1
      requests:
        cpu: "6"
        memory: 64Gi
        nvidia.com/gpu: 1
    securityContext:
      runAsUser: 4474987
    volumeMounts:
    - mountPath: /dev/shm
      name: dshm
  {{- with .Values.imagePullSecrets }}
  imagePullSecrets:
    {{- toYaml . | nindent 8 }}
  {{- end }}
  protocolVersions:
  - v2
  - grpc-v2
  supportedModelFormats:
  - autoSelect: true
    name: nvidia-nim-llm
    priority: 1
    version: 24.02.day0
  volumes:
  - emptyDir:
      medium: Memory
      sizeLimit: 128Gi
    name: dshm
