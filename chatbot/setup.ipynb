{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This notebook will guide you through the setup process for the demo. The setup\n",
    "consists of the following steps:\n",
    "\n",
    "1. **Create Kubernetes Secrets**: These secrets will allow you to download\n",
    "   models from the Hugging Face Hub and NVIDIA NGC, as well as pull the\n",
    "   necessary images from NVIDIA NGC.\n",
    "1. **Create a Persistent Volume Claim (PVC)**: This will provide storage for the\n",
    "   models.\n",
    "1. **Run Kubernetes Jobs**: These jobs will download the models to the PVC.\n",
    "\n",
    "Let's begin by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import getpass\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Kubernetes Secrets\n",
    "\n",
    "The following code will create the necessary Kubernetes secrets to download the\n",
    "models and images. Ensure you have your own API keys for the Hugging Face Hub\n",
    "and NVIDIA NGC ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGC_API_KEY = getpass.getpass(\"Enter your NGC API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_API_KEY = getpass.getpass(\"Enter your Hugging Face API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to create the following secrets:\n",
    "\n",
    "- **NGC CLI Secret**: This secret will store the NGC API Key, which you will use\n",
    "  to download the Embeddings model.\n",
    "- **Hugging Face CLI Secret**: This secret will store the Hugging Face API Key,\n",
    "  which you will use to download the LLM.\n",
    "- **Image Pull Secret**: This secret will enable you to download the\n",
    "  NVIDIA-specific containers needed throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_cli_secret = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: ngc-cli-secret\n",
    "type: Opaque\n",
    "data:\n",
    "  NGC_CLI_API_KEY: {0}\n",
    "\"\"\".format(base64.b64encode(NGC_API_KEY.encode()).decode())\n",
    "\n",
    "with open(\"ngc-cli-secret.yaml\", \"w\") as f:\n",
    "    f.write(ngc_cli_secret)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"ngc-cli-secret.yaml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_cli_secret = \"\"\"apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: hf-cli-secret\n",
    "type: Opaque\n",
    "data:\n",
    "  HF_CLI_API_KEY: {0}\n",
    "\"\"\".format(base64.b64encode(HF_API_KEY.encode()).decode())\n",
    "\n",
    "with open(\"hf-cli-secret.yaml\", \"w\") as f:\n",
    "    f.write(hf_cli_secret)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"hf-cli-secret.yaml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NVCR_SECRET = \"\"\"\n",
    "{{\"auths\":{{\"nvcr.io\":{{\"username\":\"$oauthtoken\",\"password\":\"{0}\"}}}}}}\n",
    "\"\"\"\n",
    "\n",
    "ngc_secret = \"\"\"apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: ngc-secret\n",
    "type: kubernetes.io/dockerconfigjson\n",
    "data:\n",
    "  .dockerconfigjson: {0}\n",
    "\"\"\".format(base64.b64encode(NVCR_SECRET.format(NGC_API_KEY).encode()).decode())\n",
    "\n",
    "with open(\"ngc-secret.yaml\", \"w\") as f:\n",
    "    f.write(ngc_secret)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"ngc-secret.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Persistent Volume Claim (PVC)\n",
    "\n",
    "Next, you will create a Persistent Volume Claim (PVC) to store the models you\n",
    "download. This PVC will be mounted to the containers used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repository = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: model-repository\n",
    "spec:\n",
    "  accessModes:\n",
    "  - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 500Gi\n",
    "  storageClassName: dataplatform\n",
    "\"\"\"\n",
    "\n",
    "with open(\"model-repository.yaml\", \"w\") as f:\n",
    "    f.write(model_repository)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"model-repository.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Models\n",
    "\n",
    "Finally, you will download the two models you need for this tutorial:\n",
    "\n",
    "- **Embeddings Model**: This model will embed user queries, enabling the agent\n",
    "  to retrieve the most relevant information from the vector store.\n",
    "- **Large Language Model (LLM)**: This model will support the agent's\n",
    "  decision-making processes and generate responses for the chatbot.\n",
    "\n",
    "Running the following Kubernetes Jobs will download the models to the PVC\n",
    "automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngc_downloader_job = \"\"\"apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: ngc-downloader\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: ngc-downloader\n",
    "        image: nvcr.io/ohlfw0olaadg/ea-participants/ngc-cli:v3.41.2\n",
    "        command:\n",
    "        - \"/bin/sh\"\n",
    "        - \"-c\"\n",
    "        - |\n",
    "          echo \"Creating directory /mnt/model-repo/model-store\"\n",
    "          mkdir -p /mnt/model-repo/model-store\n",
    "          echo \"Downloading embeddings model...\"\n",
    "          ngc registry model download-version --dest /mnt/model-repo/model-store ohlfw0olaadg/ea-participants/nv-embed-qa:4\n",
    "          echo \"Model downloaded successfully.\"\n",
    "        volumeMounts:\n",
    "        - name: model-volume\n",
    "          mountPath: /mnt/model-repo\n",
    "        env:\n",
    "        - name: NGC_CLI_API_KEY\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: ngc-cli-secret\n",
    "              key: NGC_CLI_API_KEY\n",
    "        - name: NGC_CLI_ORG\n",
    "          value: \"nemo-microservice (ohlfw0olaadg)\"\n",
    "        securityContext:\n",
    "          runAsUser: 0\n",
    "      restartPolicy: Never\n",
    "      imagePullSecrets:\n",
    "      - name: ngc-secret\n",
    "      volumes:\n",
    "      - name: model-volume\n",
    "        persistentVolumeClaim:\n",
    "          claimName: model-repository\n",
    "\"\"\"\n",
    "\n",
    "with open(\"ngc-downloader-job.yaml\", \"w\") as f:\n",
    "    f.write(ngc_downloader_job)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"ngc-downloader-job.yaml\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_downloader_job = \"\"\"apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: hf-downloader\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: hf-downloader\n",
    "        image: python:3.12.3\n",
    "        command:\n",
    "        - \"/bin/sh\"\n",
    "        - \"-c\"\n",
    "        - |\n",
    "          echo \"Installing the HuggingFace CLI...\"\n",
    "          pip install -U huggingface_hub[cli]\n",
    "          echo \"Creating directory /mnt/model-repo/Llama-2-7b-chat-hf\"\n",
    "          mkdir -p /mnt/model-repo/Llama-2-7b-chat-hf\n",
    "          echo \"Downloading Llama 2 7B Chat from HuggingFace HUB...\"\n",
    "          huggingface-cli download meta-llama/Llama-2-7b-chat-hf --local-dir /mnt/model-repo/Llama-2-7b-chat-hf\n",
    "          echo \"Model downloaded successfully.\"\n",
    "        volumeMounts:\n",
    "        - name: model-volume\n",
    "          mountPath: /mnt/model-repo\n",
    "        env:\n",
    "        - name: HF_TOKEN\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: hf-cli-secret\n",
    "              key: HF_CLI_API_KEY\n",
    "        securityContext:\n",
    "          runAsUser: 0\n",
    "      restartPolicy: Never\n",
    "      volumes:\n",
    "      - name: model-volume\n",
    "        persistentVolumeClaim:\n",
    "          claimName: model-repository\n",
    "\"\"\"\n",
    "\n",
    "with open(\"hf-downloader-job.yaml\", \"w\") as f:\n",
    "    f.write(hf_downloader_job)\n",
    "\n",
    "subprocess.run([\"kubectl\", \"apply\", \"-f\", \"hf-downloader-job.yaml\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "You are now ready to deploy the demo. Use the provided Helm chart to install the\n",
    "application using the **Import Framework** wizard in the Unified Analytics\n",
    "Platform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
